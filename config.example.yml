paths:
  # The directories can be either relative or absolute path.
  assets-dir: 'assets'
  binaries-dir: 'binaries'
  git-clones-dir: 'git-clones'

general:
  enable-on-finish: true
  on-finish:
    # execute llama-swap and get ready for connections
    - "./binaries/llama-swap/release/llama-swap -config /app/configs/llama-swap-config.yml"
  GitHub:
    gitToken: ''
    gitRateLimit:
      count: 10 # do not make more than X call per [timeout] minutes
      timeout: 60

builds: # List of GitHub repos
  # Inference of Meta's LLaMA model (and others) in pure C/C++
  llama-swap:
    enabled:
      true
    check-updates:
      true
    update-type: 'git-binary'
    git-release:
      url: 'https://api.github.com/repos/mostlygeek/llama-swap/releases'
      type: 'linux'
      subtype: 'general'
      archive-pattern:
        'linux':
          binaries: # These entries will be marked as executables.
            'server': 'build/bin/llama-server'
          subtypes:
            'general': 'llama-swap_*_linux_amd64.tar.gz'



  # whisply combines faster-whisper and insanely-fast-whisper to offer an easy-to-use solution for batch processing files on Windows, Linux and Mac. It also enables word-level speaker annotation by integrating whisperX and pyannote.
  whisply:
    enabled:
      true
    check-updates:
      true
    update-type: 'git-sourcecode'
    options:
      - python-venv
    post-clone-commands:
      - 'pip install .'
    post-update-commands:
      - 'pip install .'
    git-sourcecode:
      'repo': 'https://github.com/tsmdt/whisply'
      'branch': 'main'

  llamacpp:
    enabled:
      true
    check-updates:
      true
    update-type: 'git-sourcecode'
    post-clone-commands:
      - 'cmake -B build -DGGML_CUDA=ON && cmake --build build --config Release -j 16 || true' # CUDA only build
    post-update-commands:
      - 'cmake -B build -DGGML_CUDA=ON && cmake --build build --config Release -j 16 || true' # CUDA only build
    git-sourcecode:
      'repo': 'https://github.com/ggml-org/llama.cpp.git'
      'branch': 'master'


  # whisply combines faster-whisper and insanely-fast-whisper to offer an easy-to-use solution for batch processing files on Windows, Linux and Mac. It also enables word-level speaker annotation by integrating whisperX and pyannote.
  whispercpp:
    enabled:
      true
    check-updates:
      true
    update-type: 'git-sourcecode'
    post-clone-commands:
      - 'cmake -B build -DWHISPER_SDL2=ON && cmake --build build --config Release -j 16 || true' # SDL2 only build
      - 'sh ./models/download-ggml-model.sh tiny'
      - 'sh ./models/download-ggml-model.sh tiny.en'
      - 'sh ./models/download-ggml-model.sh small'
      - 'sh ./models/download-ggml-model.sh small.en'
      - 'sh ./models/download-ggml-model.sh small.en-tdrz'
      - 'sh ./models/download-ggml-model.sh base'
      - 'sh ./models/download-ggml-model.sh base.en'
      - 'sh ./models/download-ggml-model.sh medium'
      - 'sh ./models/download-ggml-model.sh medium.en'
      - 'sh ./models/download-ggml-model.sh large-v1'
      - 'sh ./models/download-ggml-model.sh large-v2'
      - 'sh ./models/download-ggml-model.sh large-v3'
      - 'sh ./models/download-ggml-model.sh large-v3-turbo'
    post-update-commands:
      - 'cmake -B build -DWHISPER_SDL2=ON && cmake --build build --config Release -j 16 || true' # SDL only build
    git-sourcecode:
      'repo': 'https://github.com/ggerganov/whisper.cpp'
      'branch': 'master'

