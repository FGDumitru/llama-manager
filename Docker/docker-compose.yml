services:
  llama-manager:
    platform: linux/amd64
    container_name: llama-manager
    hostname: llama-manager
#    restart: unless-stopped
    tty: true
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ../:/app
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: 1
              capabilities: [ gpu ]
