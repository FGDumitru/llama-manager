services:
  llama-manager:
    platform: linux/amd64
    container_name: llama-manager
    hostname: llama-manager
#    restart: unless-stopped
    tty: true
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8855:8080"
      - "8866:8081"
      - "9966:8999"
    volumes:
      - ../:/app
      - ./models:/models
      - ../webui_data:/miniconda3/envs/open-web_venv/lib/python3.11/site-packages/open_webui/data
