services:
  llama-manager:
    platform: linux/amd64
    container_name: llama-manager
    hostname: llama-manager
#    restart: unless-stopped
    tty: true
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8855:8080"
      - "9911:9911"
    volumes:
      - ../:/app
      - /home/panthera/NVME4TB/models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: 1
              capabilities: [ gpu ]
