# Seconds to wait for llama.cpp to load and be ready to serve requests
# Default (and minimum) is 15 seconds
healthCheckTimeout: 3600

# Write HTTP logs (useful for troubleshooting), defaults to false
logRequests: true

# define valid model values and the upstream server start
models:

  "[NO GPU] Meta-Llama-3.1-8B-Instruct-240807-XelotX-Q4_K_M":
    cmd: >
      /app/git-clones/llamacpp/llamacpp/build/bin/llama-server 
      --port 8999 
      -m /app/models/Meta-Llama-3.1-8B-Instruct-240807-XelotX-Q4_K_M.gguf
      -fa
      -c 32768
      -ctk q4_0 -ctv q4_0
    # where to reach the server started by cmd, make sure the ports match
    proxy: http://0.0.0.0:8999
    # aliases names to use this model for
    aliases:
      - "nogpu-llama-3.1-8b.q4km"
    checkEndpoint: /health
    ttl: 3600


  "[NVIDIA GPU] Meta-Llama-3.1-8B-Instruct-240807-XelotX-Q4_K_M":
    cmd: >
      /app/git-clones/llamacpp/llamacpp/build/bin/llama-server 
      --port 8999 
      -m /app/models/Meta-Llama-3.1-8B-Instruct-240807-XelotX-Q4_K_M.gguf
      -ngl 81 
      -fa
      -c 32768
      -ctk q4_0 -ctv q4_0
    # where to reach the server started by cmd, make sure the ports match
    proxy: http://0.0.0.0:8999
    # aliases names to use this model for
    aliases:
      - "nogpu-llama-3.1-8b.q4km"
    checkEndpoint: /health
    ttl: 3600

